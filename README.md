# ‚öΩ Pipeline de Datos de Ligas de F√∫tbol

Proyecto **end-to-end de Ingenier√≠a de Datos** utilizando **Apache Airflow, Docker y Snowflake**, enfocado en la extracci√≥n, transformaci√≥n y carga (ETL) de datos de posiciones de ligas de f√∫tbol.

---

## üöÄ Stack Tecnol√≥gico

- **Apache Airflow 2** (orquestaci√≥n de workflows)
- **Docker** (entorno reproducible)
- **Astronomer Runtime**
- **Snowflake** (Data Warehouse)
- **Python** (Pandas)
- **SQL**

---

## üèóÔ∏è Arquitectura del Pipeline

1. **Extracci√≥n**  
   Obtenci√≥n de datos de posiciones de ligas de f√∫tbol desde fuentes p√∫blicas.

2. **Transformaci√≥n**  
   Limpieza y transformaci√≥n de los datos usando Pandas:
   - Normalizaci√≥n de columnas
   - Enriquecimiento con informaci√≥n de equipos
   - Generaci√≥n de dataset final

3. **Carga**
   - Exportaci√≥n de datos a CSV
   - Carga del archivo a un **Stage en Snowflake**
   - Inserci√≥n de datos en tablas finales mediante SQL

Todo el proceso est√° orquestado mediante un **DAG de Airflow**.

---

## ‚ñ∂Ô∏è Ejecuci√≥n Local

### Requisitos
- Docker
- Astro CLI
- Cuenta en Snowflake

### Pasos

1. Clonar el repositorio:

git clone https://github.com/HiIsai/airflow-football-pipeline.git
cd airflow-football-pipeline

2. Crear el archivo de variables de entorno:

cp .env.example .env


3. Iniciar Airflow:

astro dev start --no-cache


4. Acceder a Airflow:

http://localhost:8080